{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "\n",
    "pd.set_option(\"max_columns\", 300)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "os.chdir(os.getcwd().replace('notebooks','').replace('medi_crawler',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as CONFIG\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with features and labels\n",
    "X = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/labeled_data.csv\")['Title'])\n",
    "y = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/labeled_data.csv\")['label'])\n",
    "\n",
    "bool_not_nan = ~X['Title'].isna()\n",
    "X = X[bool_not_nan]\n",
    "y = y[bool_not_nan]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 30\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert X_train and X_test to list of strings\n",
    "X_train = X_train.squeeze().tolist()\n",
    "X_test = X_test.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenizer object\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Tokenize the text\n",
    "train_encodings = tokenizer(X_train,\n",
    "                            truncation=True,\n",
    "                            padding=True,\n",
    "                            max_length=MAX_LEN,\n",
    "                            return_tensors='tf')  # Convert to TensorFlow tensors\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(X_test,\n",
    "                           truncation=True,\n",
    "                           padding=True,\n",
    "                           max_length=MAX_LEN,\n",
    "                           return_tensors='tf')  # Convert to TensorFlow tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), list(y_train.values)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), list(y_test.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n",
    "                                    list(y_train.values)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n",
    "                                    list(y_test.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Recall(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='recall', **kwargs):\n",
    "        super(Recall, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "        true_positives = tf.cast(tf.math.count_nonzero(y_true * y_pred), tf.float32)\n",
    "        false_negatives = tf.cast(tf.math.count_nonzero(y_true * (1 - y_pred)), tf.float32)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.false_negatives.assign_add(false_negatives)\n",
    "    \n",
    "    def result(self):\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0.0)\n",
    "        self.false_negatives.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "24/24 [==============================] - 80s 3s/step - loss: 0.6672 - recall: 0.0483\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\keras\\engine\\training.py:2416: UserWarning: Metric Recall implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 50s 2s/step - loss: 0.5584 - recall: 0.2650\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.4978 - recall: 0.4417\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.2256 - recall: 0.4533\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.1357 - recall: 0.4517\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.1069 - recall: 0.4617\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - 51s 2s/step - loss: 0.0400 - recall: 0.4517\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - 51s 2s/step - loss: 0.0449 - recall: 0.4583\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.0452 - recall: 0.5150\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.1612 - recall: 0.4117\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.0738 - recall: 0.5200\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0208 - recall: 0.4400\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.0072 - recall: 0.4750\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.0034 - recall: 0.4550\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0024 - recall: 0.4717\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0020 - recall: 0.4717\n",
      "Epoch 17/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0016 - recall: 0.4683\n",
      "Epoch 18/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 0.0014 - recall: 0.4517\n",
      "Epoch 19/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0012 - recall: 0.4750\n",
      "Epoch 20/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 0.0012 - recall: 0.4550\n",
      "Epoch 21/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 9.1156e-04 - recall: 0.4950\n",
      "Epoch 22/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 7.9844e-04 - recall: 0.4483\n",
      "Epoch 23/30\n",
      "24/24 [==============================] - 48s 2s/step - loss: 6.9676e-04 - recall: 0.4817\n",
      "Epoch 24/30\n",
      "24/24 [==============================] - 50s 2s/step - loss: 6.6078e-04 - recall: 0.4717\n",
      "Epoch 25/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 5.6834e-04 - recall: 0.4583\n",
      "Epoch 26/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 5.4765e-04 - recall: 0.4417\n",
      "Epoch 27/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 4.9891e-04 - recall: 0.4350\n",
      "Epoch 28/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 4.5393e-04 - recall: 0.4750\n",
      "Epoch 29/30\n",
      "24/24 [==============================] - 51s 2s/step - loss: 4.0876e-04 - recall: 0.4717\n",
      "Epoch 30/30\n",
      "24/24 [==============================] - 49s 2s/step - loss: 3.5285e-04 - recall: 0.4450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d22df3e358>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model and tokenizer\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Choose the optimizer\n",
    "optimizerr = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#build the model\n",
    "model.compile(optimizer=optimizerr,\n",
    "              loss=losss,\n",
    "              metrics=[Recall()])\n",
    "# train the model \n",
    "model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),\n",
    "          epochs=N_EPOCHS,\n",
    "          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...classifier\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\n",
      "......vars\n",
      "...distilbert\\embeddings\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\embeddings\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\embeddings\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...dropout\n",
      "......vars\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\recall\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........100\n",
      ".........101\n",
      ".........102\n",
      ".........103\n",
      ".........104\n",
      ".........105\n",
      ".........106\n",
      ".........107\n",
      ".........108\n",
      ".........109\n",
      ".........11\n",
      ".........110\n",
      ".........111\n",
      ".........112\n",
      ".........113\n",
      ".........114\n",
      ".........115\n",
      ".........116\n",
      ".........117\n",
      ".........118\n",
      ".........119\n",
      ".........12\n",
      ".........120\n",
      ".........121\n",
      ".........122\n",
      ".........123\n",
      ".........124\n",
      ".........125\n",
      ".........126\n",
      ".........127\n",
      ".........128\n",
      ".........129\n",
      ".........13\n",
      ".........130\n",
      ".........131\n",
      ".........132\n",
      ".........133\n",
      ".........134\n",
      ".........135\n",
      ".........136\n",
      ".........137\n",
      ".........138\n",
      ".........139\n",
      ".........14\n",
      ".........140\n",
      ".........141\n",
      ".........142\n",
      ".........143\n",
      ".........144\n",
      ".........145\n",
      ".........146\n",
      ".........147\n",
      ".........148\n",
      ".........149\n",
      ".........15\n",
      ".........150\n",
      ".........151\n",
      ".........152\n",
      ".........153\n",
      ".........154\n",
      ".........155\n",
      ".........156\n",
      ".........157\n",
      ".........158\n",
      ".........159\n",
      ".........16\n",
      ".........160\n",
      ".........161\n",
      ".........162\n",
      ".........163\n",
      ".........164\n",
      ".........165\n",
      ".........166\n",
      ".........167\n",
      ".........168\n",
      ".........169\n",
      ".........17\n",
      ".........170\n",
      ".........171\n",
      ".........172\n",
      ".........173\n",
      ".........174\n",
      ".........175\n",
      ".........176\n",
      ".........177\n",
      ".........178\n",
      ".........179\n",
      ".........18\n",
      ".........180\n",
      ".........181\n",
      ".........182\n",
      ".........183\n",
      ".........184\n",
      ".........185\n",
      ".........186\n",
      ".........187\n",
      ".........188\n",
      ".........189\n",
      ".........19\n",
      ".........190\n",
      ".........191\n",
      ".........192\n",
      ".........193\n",
      ".........194\n",
      ".........195\n",
      ".........196\n",
      ".........197\n",
      ".........198\n",
      ".........199\n",
      ".........2\n",
      ".........20\n",
      ".........200\n",
      ".........201\n",
      ".........202\n",
      ".........203\n",
      ".........204\n",
      ".........205\n",
      ".........206\n",
      ".........207\n",
      ".........208\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........29\n",
      ".........3\n",
      ".........30\n",
      ".........31\n",
      ".........32\n",
      ".........33\n",
      ".........34\n",
      ".........35\n",
      ".........36\n",
      ".........37\n",
      ".........38\n",
      ".........39\n",
      ".........4\n",
      ".........40\n",
      ".........41\n",
      ".........42\n",
      ".........43\n",
      ".........44\n",
      ".........45\n",
      ".........46\n",
      ".........47\n",
      ".........48\n",
      ".........49\n",
      ".........5\n",
      ".........50\n",
      ".........51\n",
      ".........52\n",
      ".........53\n",
      ".........54\n",
      ".........55\n",
      ".........56\n",
      ".........57\n",
      ".........58\n",
      ".........59\n",
      ".........6\n",
      ".........60\n",
      ".........61\n",
      ".........62\n",
      ".........63\n",
      ".........64\n",
      ".........65\n",
      ".........66\n",
      ".........67\n",
      ".........68\n",
      ".........69\n",
      ".........7\n",
      ".........70\n",
      ".........71\n",
      ".........72\n",
      ".........73\n",
      ".........74\n",
      ".........75\n",
      ".........76\n",
      ".........77\n",
      ".........78\n",
      ".........79\n",
      ".........8\n",
      ".........80\n",
      ".........81\n",
      ".........82\n",
      ".........83\n",
      ".........84\n",
      ".........85\n",
      ".........86\n",
      ".........87\n",
      ".........88\n",
      ".........89\n",
      ".........9\n",
      ".........90\n",
      ".........91\n",
      ".........92\n",
      ".........93\n",
      ".........94\n",
      ".........95\n",
      ".........96\n",
      ".........97\n",
      ".........98\n",
      ".........99\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-07-12 21:18:01         2053\n",
      "metadata.json                                  2023-07-12 21:18:01           64\n",
      "variables.h5                                   2023-07-12 21:18:08    803776672\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open('recall_trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 6s 61ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.66      0.78        35\n",
      "        True       0.50      0.92      0.65        13\n",
      "\n",
      "    accuracy                           0.73        48\n",
      "   macro avg       0.73      0.79      0.71        48\n",
      "weighted avg       0.83      0.73      0.74        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict Test data\n",
    "preds = model.predict(test_dataset).logits\n",
    "\n",
    "preds = tf.nn.softmax(preds, axis=1).numpy()  \n",
    "\n",
    "y_preds = np.round(preds[:,1],0)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have the predicted labels 'y_pred' and the true labels 'y_true'\n",
    "report = classification_report(y_test, y_preds)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_df = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/unlabeled_data.csv\")['Title'])\n",
    "pmid_unlabeld = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/unlabeled_data.csv\")['pmid'])\n",
    "\n",
    "unlabeled_not_nan = ~X_new_df['Title'].isna()\n",
    "\n",
    "X_new_df = X_new_df[unlabeled_not_nan]\n",
    "pmid_unlabeld = pmid_unlabeld[unlabeled_not_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4738/4738 [==============================] - 666s 140ms/step\n"
     ]
    }
   ],
   "source": [
    "X_new = X_new_df.squeeze().values\n",
    "\n",
    "encodings = tokenizer(X_new.squeeze().tolist(), \n",
    "                      max_length=MAX_LEN, \n",
    "                      truncation=True, \n",
    "                      padding=True)\n",
    "# Transform to tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(dataset).logits\n",
    "\n",
    "preds = tf.nn.softmax(preds, axis=1).numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>Right ventricle to pulmonary artery conduit has a favorable impact on postoperative physiology after Stage I Norwood: preliminary results.</td>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>[Further development of pulmonary artery after Glenn procedure: effect of different antegrade pulmonary blood flows on cyanotic congenital heart defects].</td>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>Pulmonary artery size at the time of bidirectional cavopulmonary shunt and Fontan surgery influences long-term outcomes.</td>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>Changes in pulmonary artery size before and after total cavopulmonary connection.</td>\n",
       "      <td>0.999815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>Impact of shunt type on growth of pulmonary arteries after norwood stage I procedure: current best available evidence.</td>\n",
       "      <td>0.999815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                           Title  \\\n",
       "3056                  Right ventricle to pulmonary artery conduit has a favorable impact on postoperative physiology after Stage I Norwood: preliminary results.   \n",
       "2350  [Further development of pulmonary artery after Glenn procedure: effect of different antegrade pulmonary blood flows on cyanotic congenital heart defects].   \n",
       "1864                                    Pulmonary artery size at the time of bidirectional cavopulmonary shunt and Fontan surgery influences long-term outcomes.   \n",
       "3551                                                                           Changes in pulmonary artery size before and after total cavopulmonary connection.   \n",
       "2058                                      Impact of shunt type on growth of pulmonary arteries after norwood stage I procedure: current best available evidence.   \n",
       "\n",
       "          pred  \n",
       "3056  0.999816  \n",
       "2350  0.999816  \n",
       "1864  0.999816  \n",
       "3551  0.999815  \n",
       "2058  0.999815  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.concat([X_new_df,pd.DataFrame(preds[:,1],index=X_new_df.index,columns=['pred'])],axis=1).sort_values('pred',ascending=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('data/medi_crawler/final/predict_label_title_recall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(text_list, model, tokenizer):  \n",
    "    #tokenize the text\n",
    "    encodings = tokenizer(text_list, \n",
    "                          max_length=MAX_LEN, \n",
    "                          truncation=True, \n",
    "                          padding=True)\n",
    "    #transform to tf.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "    #predict\n",
    "    preds = model.predict(dataset.batch(1)).logits  \n",
    "    \n",
    "    #transform to array with probabilities\n",
    "    res = tf.nn.softmax(preds, axis=1).numpy()      \n",
    "    \n",
    "    return res\n",
    "\n",
    "predict_proba(strings_list[3], model, tokenizer)[:,1].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
