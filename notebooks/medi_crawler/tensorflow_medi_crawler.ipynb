{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "pd.set_option(\"max_columns\", 300)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "os.chdir(os.getcwd().replace('notebooks','').replace('medi_crawler',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as CONFIG\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with features and labels\n",
    "X = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/labeled_data.csv\")['Title'])\n",
    "y = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/labeled_data.csv\")['label'])\n",
    "\n",
    "bool_not_nan = ~X['Title'].isna()\n",
    "X = X[bool_not_nan]\n",
    "y = y[bool_not_nan]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 30\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert X_train and X_test to list of strings\n",
    "X_train = X_train.squeeze().tolist()\n",
    "X_test = X_test.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenizer object\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Tokenize the text\n",
    "train_encodings = tokenizer(X_train,\n",
    "                            truncation=True,\n",
    "                            padding=True,\n",
    "                            max_length=MAX_LEN,\n",
    "                            return_tensors='tf')  # Convert to TensorFlow tensors\n",
    "\n",
    "\n",
    "test_encodings = tokenizer(X_test,\n",
    "                           truncation=True,\n",
    "                           padding=True,\n",
    "                           max_length=MAX_LEN,\n",
    "                           return_tensors='tf')  # Convert to TensorFlow tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), list(y_train.values)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), list(y_test.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n",
    "                                    list(y_train.values)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n",
    "                                    list(y_test.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Recall(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='recall', **kwargs):\n",
    "        super(Recall, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "        true_positives = tf.cast(tf.math.count_nonzero(y_true * y_pred), tf.float32)\n",
    "        false_negatives = tf.cast(tf.math.count_nonzero(y_true * (1 - y_pred)), tf.float32)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.false_negatives.assign_add(false_negatives)\n",
    "    \n",
    "    def result(self):\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0.0)\n",
    "        self.false_negatives.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "24/24 [==============================] - 79s 2s/step - loss: 0.8291 - recall: 0.2033\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\keras\\engine\\training.py:2416: UserWarning: Metric Recall implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 52s 2s/step - loss: 0.5344 - recall: 0.3200\n",
      "Epoch 3/30\n",
      "24/24 [==============================] - 52s 2s/step - loss: 0.3678 - recall: 0.4283\n",
      "Epoch 4/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 0.1297 - recall: 0.4383\n",
      "Epoch 5/30\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.1067 - recall: 0.4433\n",
      "Epoch 6/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 0.1125 - recall: 0.4367\n",
      "Epoch 7/30\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.0548 - recall: 0.4667\n",
      "Epoch 8/30\n",
      "24/24 [==============================] - 54s 2s/step - loss: 0.0121 - recall: 0.4633\n",
      "Epoch 9/30\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.0018 - recall: 0.4883\n",
      "Epoch 10/30\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0833 - recall: 0.5133\n",
      "Epoch 11/30\n",
      "24/24 [==============================] - 54s 2s/step - loss: 0.0204 - recall: 0.4483\n",
      "Epoch 12/30\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.0353 - recall: 0.4800\n",
      "Epoch 13/30\n",
      "24/24 [==============================] - 55s 2s/step - loss: 0.0078 - recall: 0.4783\n",
      "Epoch 14/30\n",
      "24/24 [==============================] - 55s 2s/step - loss: 0.0054 - recall: 0.4517\n",
      "Epoch 15/30\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.0017 - recall: 0.4683\n",
      "Epoch 16/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 9.9710e-04 - recall: 0.4850\n",
      "Epoch 17/30\n",
      "24/24 [==============================] - 52s 2s/step - loss: 6.2224e-04 - recall: 0.4683\n",
      "Epoch 18/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 4.4769e-04 - recall: 0.4983\n",
      "Epoch 19/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 3.8504e-04 - recall: 0.4583\n",
      "Epoch 20/30\n",
      "24/24 [==============================] - 54s 2s/step - loss: 3.9189e-04 - recall: 0.4717\n",
      "Epoch 21/30\n",
      "24/24 [==============================] - 54s 2s/step - loss: 3.1163e-04 - recall: 0.4717\n",
      "Epoch 22/30\n",
      "24/24 [==============================] - 56s 2s/step - loss: 5.7803e-04 - recall: 0.4517\n",
      "Epoch 23/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 3.9069e-04 - recall: 0.4883\n",
      "Epoch 24/30\n",
      "24/24 [==============================] - 52s 2s/step - loss: 2.5289e-04 - recall: 0.5150\n",
      "Epoch 25/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 2.9564e-04 - recall: 0.4850\n",
      "Epoch 26/30\n",
      "24/24 [==============================] - 52s 2s/step - loss: 2.0841e-04 - recall: 0.4483\n",
      "Epoch 27/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 1.9332e-04 - recall: 0.4750\n",
      "Epoch 28/30\n",
      "24/24 [==============================] - 52s 2s/step - loss: 1.6100e-04 - recall: 0.4550\n",
      "Epoch 29/30\n",
      "24/24 [==============================] - 53s 2s/step - loss: 1.4894e-04 - recall: 0.4383\n",
      "Epoch 30/30\n",
      "24/24 [==============================] - 52s 2s/step - loss: 1.4317e-04 - recall: 0.4750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x203dda97a20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "#chose the optimizer\n",
    "optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "#define the loss function \n",
    "losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#build the model\n",
    "model.compile(optimizer=optimizerr,\n",
    "              loss=losss,\n",
    "              metrics=[Recall()])\n",
    "# train the model \n",
    "model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),\n",
    "          epochs=N_EPOCHS,\n",
    "          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...classifier\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\n",
      "......vars\n",
      "...distilbert\\embeddings\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\embeddings\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\embeddings\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...dropout\n",
      "......vars\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\recall\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........100\n",
      ".........101\n",
      ".........102\n",
      ".........103\n",
      ".........104\n",
      ".........105\n",
      ".........106\n",
      ".........107\n",
      ".........108\n",
      ".........109\n",
      ".........11\n",
      ".........110\n",
      ".........111\n",
      ".........112\n",
      ".........113\n",
      ".........114\n",
      ".........115\n",
      ".........116\n",
      ".........117\n",
      ".........118\n",
      ".........119\n",
      ".........12\n",
      ".........120\n",
      ".........121\n",
      ".........122\n",
      ".........123\n",
      ".........124\n",
      ".........125\n",
      ".........126\n",
      ".........127\n",
      ".........128\n",
      ".........129\n",
      ".........13\n",
      ".........130\n",
      ".........131\n",
      ".........132\n",
      ".........133\n",
      ".........134\n",
      ".........135\n",
      ".........136\n",
      ".........137\n",
      ".........138\n",
      ".........139\n",
      ".........14\n",
      ".........140\n",
      ".........141\n",
      ".........142\n",
      ".........143\n",
      ".........144\n",
      ".........145\n",
      ".........146\n",
      ".........147\n",
      ".........148\n",
      ".........149\n",
      ".........15\n",
      ".........150\n",
      ".........151\n",
      ".........152\n",
      ".........153\n",
      ".........154\n",
      ".........155\n",
      ".........156\n",
      ".........157\n",
      ".........158\n",
      ".........159\n",
      ".........16\n",
      ".........160\n",
      ".........161\n",
      ".........162\n",
      ".........163\n",
      ".........164\n",
      ".........165\n",
      ".........166\n",
      ".........167\n",
      ".........168\n",
      ".........169\n",
      ".........17\n",
      ".........170\n",
      ".........171\n",
      ".........172\n",
      ".........173\n",
      ".........174\n",
      ".........175\n",
      ".........176\n",
      ".........177\n",
      ".........178\n",
      ".........179\n",
      ".........18\n",
      ".........180\n",
      ".........181\n",
      ".........182\n",
      ".........183\n",
      ".........184\n",
      ".........185\n",
      ".........186\n",
      ".........187\n",
      ".........188\n",
      ".........189\n",
      ".........19\n",
      ".........190\n",
      ".........191\n",
      ".........192\n",
      ".........193\n",
      ".........194\n",
      ".........195\n",
      ".........196\n",
      ".........197\n",
      ".........198\n",
      ".........199\n",
      ".........2\n",
      ".........20\n",
      ".........200\n",
      ".........201\n",
      ".........202\n",
      ".........203\n",
      ".........204\n",
      ".........205\n",
      ".........206\n",
      ".........207\n",
      ".........208\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........29\n",
      ".........3\n",
      ".........30\n",
      ".........31\n",
      ".........32\n",
      ".........33\n",
      ".........34\n",
      ".........35\n",
      ".........36\n",
      ".........37\n",
      ".........38\n",
      ".........39\n",
      ".........4\n",
      ".........40\n",
      ".........41\n",
      ".........42\n",
      ".........43\n",
      ".........44\n",
      ".........45\n",
      ".........46\n",
      ".........47\n",
      ".........48\n",
      ".........49\n",
      ".........5\n",
      ".........50\n",
      ".........51\n",
      ".........52\n",
      ".........53\n",
      ".........54\n",
      ".........55\n",
      ".........56\n",
      ".........57\n",
      ".........58\n",
      ".........59\n",
      ".........6\n",
      ".........60\n",
      ".........61\n",
      ".........62\n",
      ".........63\n",
      ".........64\n",
      ".........65\n",
      ".........66\n",
      ".........67\n",
      ".........68\n",
      ".........69\n",
      ".........7\n",
      ".........70\n",
      ".........71\n",
      ".........72\n",
      ".........73\n",
      ".........74\n",
      ".........75\n",
      ".........76\n",
      ".........77\n",
      ".........78\n",
      ".........79\n",
      ".........8\n",
      ".........80\n",
      ".........81\n",
      ".........82\n",
      ".........83\n",
      ".........84\n",
      ".........85\n",
      ".........86\n",
      ".........87\n",
      ".........88\n",
      ".........89\n",
      ".........9\n",
      ".........90\n",
      ".........91\n",
      ".........92\n",
      ".........93\n",
      ".........94\n",
      ".........95\n",
      ".........96\n",
      ".........97\n",
      ".........98\n",
      ".........99\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-07-12 21:18:01         2053\n",
      "metadata.json                                  2023-07-12 21:18:01           64\n",
      "variables.h5                                   2023-07-12 21:18:08    803776672\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open('recall_trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 7s 86ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.71      0.78        35\n",
      "        True       0.47      0.69      0.56        13\n",
      "\n",
      "    accuracy                           0.71        48\n",
      "   macro avg       0.67      0.70      0.67        48\n",
      "weighted avg       0.76      0.71      0.72        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict Test data\n",
    "preds = model.predict(test_dataset).logits\n",
    "\n",
    "preds = tf.nn.softmax(preds, axis=1).numpy()  \n",
    "\n",
    "y_preds = np.round(preds[:,1],0)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have the predicted labels 'y_pred' and the true labels 'y_true'\n",
    "report = classification_report(y_test, y_preds)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_df = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/unlabeled_data.csv\")['Title'])\n",
    "pmid_unlabeld = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/unlabeled_data.csv\")['pmid'])\n",
    "\n",
    "unlabeled_not_nan = ~X_new_df['Title'].isna()\n",
    "\n",
    "X_new_df = X_new_df[unlabeled_not_nan]\n",
    "pmid_unlabeld = pmid_unlabeld[unlabeled_not_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_new_df.squeeze().values\n",
    "\n",
    "encodings = tokenizer(X_new.squeeze().tolist(), \n",
    "                      max_length=MAX_LEN, \n",
    "                      truncation=True, \n",
    "                      padding=True)\n",
    "# Transform to tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(dataset).logits\n",
    "\n",
    "preds = tf.nn.softmax(preds, axis=1).numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([X_new_df,pd.DataFrame(preds[:,1],index=X_new_df.index,columns=['pred'])],axis=1).sort_values('pred',ascending=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('data/medi_crawler/final/predict_label_title_recall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(text_list, model, tokenizer):  \n",
    "    #tokenize the text\n",
    "    encodings = tokenizer(text_list, \n",
    "                          max_length=MAX_LEN, \n",
    "                          truncation=True, \n",
    "                          padding=True)\n",
    "    #transform to tf.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "    #predict\n",
    "    preds = model.predict(dataset.batch(1)).logits  \n",
    "    \n",
    "    #transform to array with probabilities\n",
    "    res = tf.nn.softmax(preds, axis=1).numpy()      \n",
    "    \n",
    "    return res\n",
    "\n",
    "predict_proba(strings_list[3], model, tokenizer)[:,1].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
