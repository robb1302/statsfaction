{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"max_columns\", 300)\n",
    "os.chdir(os.getcwd().replace('notebooks','').replace('medi_crawler',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open(\"data/medi_crawler/raw/meta_arts.pickle\", 'rb')\n",
    "object_file = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'pmid': [],\n",
    "    'text': [],\n",
    "    'title': []\n",
    "}\n",
    "\n",
    "for entry in object_file.values():\n",
    "    data['pmid'].append(entry['pmid'])\n",
    "    data['text'].append(entry['abstract'])\n",
    "    data['title'].append(entry['title'])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index('pmid')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true  labeled pearls\n",
    "\n",
    "pearls_df = pd.read_csv(\"data/medi_crawler/raw/df_pearls.csv\",sep=';',index_col=0)\n",
    "label_true = pd.DataFrame(data=[True]*len(pearls_df.columns), columns=[\"label\"], index=pearls_df.columns)\n",
    "label_true = label_true.join(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get false labeled pearls\n",
    "label_data = pd.read_csv(\"data/medi_crawler/processed/data_cleaned.csv\")\n",
    "\n",
    "\n",
    "label_false = pd.DataFrame(label_data[label_data.category!=\"pot_pearl\"]['pmid'])\n",
    "label_false = label_false.set_index(\"pmid\")\n",
    "label_false['label'] = False\n",
    "label_false = label_false.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get false data \n",
    "\n",
    "from src.medi_crawler.download import get_abstract, get_title\n",
    "abstract = get_abstract(list(label_false.index))\n",
    "title = get_title(list(label_false.index))\n",
    "\n",
    "title = title.set_index('pmid')\n",
    "abstract = abstract.set_index('pmid')\n",
    "\n",
    "label_false = label_false.join(title).join(abstract)\n",
    "label_false = label_false.join(title).join(abstract)\n",
    "\n",
    "label_false = label_false.rename(columns={'abstract': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.concat([label_true, label_false])\n",
    "model_data.to_csv('data/medi_crawler/processed/model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = df[[i not in model_data.index for i in df.index]]\n",
    "predict_data.to_csv('data/medi_crawler/processed/predict_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_false = label_false.drop(['text','title'],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 30\n",
    "MAX_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = pd.read_csv('data/medi_crawler/processed/predict_data.csv',index_col=0)[['text']]\n",
    "model_data = pd.read_csv('data/medi_crawler/processed/model_data.csv',index_col=0)[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with features and labels\n",
    "X = model_data.drop('label', axis=1)  # Features\n",
    "y = model_data['label']  # Labels\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train and X_test to list of strings\n",
    "X_train = list(X_train.squeeze().values)\n",
    "X_test = list(X_test.squeeze().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a tokenizer object\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "#tokenize the text\n",
    "train_encodings = tokenizer(list(X_train),\n",
    "                            truncation=True, \n",
    "                            padding=True)\n",
    "test_encodings = tokenizer(list(X_test),\n",
    "                           truncation=True, \n",
    "                           padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n",
    "                                    list(y_train.values)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n",
    "                                    list(y_test.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Instantiate the model and tokenizer\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Choose the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the number of epochs and batch size\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Extract the inputs and labels from the batch\n",
    "        inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update epoch statistics\n",
    "        epoch_loss += loss.item() * len(labels)\n",
    "        _, predicted_labels = torch.max(logits, 1)\n",
    "        epoch_correct += (predicted_labels == labels).sum().item()\n",
    "        total_samples += len(labels)\n",
    "\n",
    "    epoch_accuracy = epoch_correct / total_samples\n",
    "    epoch_loss /= total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "# Set the model back to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "#chose the optimizer\n",
    "optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "#define the loss function \n",
    "losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#build the model\n",
    "model.compile(optimizer=optimizerr,\n",
    "              loss=losss,\n",
    "              metrics=['accuracy'])\n",
    "# train the model \n",
    "model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),\n",
    "          epochs=N_EPOCHS,\n",
    "          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"model/medi_crawler/abstract_prediction.pickle\"\n",
    "\n",
    "# Save the classification report as a pickle file\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(test_dataset).logits\n",
    "\n",
    "preds = tf.nn.softmax(preds, axis=1).numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.round(preds[:,1],0)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have the predicted labels 'y_pred' and the true labels 'y_true'\n",
    "report = classification_report(y_test, y_preds)\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data[~predict_data['text'].isna()]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = predict_data[~predict_data['text'].isna()]['text'].squeeze().values\n",
    "\n",
    "encodings = tokenizer(X_new.squeeze().tolist(), \n",
    "                      max_length=300, \n",
    "                      truncation=True, \n",
    "                      padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform to tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(dataset).logits\n",
    "\n",
    "preds = tf.nn.softmax(preds, axis=1).numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = pd.read_csv('data/medi_crawler/processed/predict_data.csv',index_col=0)\n",
    "\n",
    "predict_data_title = predict_data[~predict_data['text'].isna()]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([predict_data_title,pd.DataFrame(preds[:,1],index=predict_data_title.index,columns=['pred'])],axis=1).sort_values('pred',ascending=False).to_csv(\"data/medi_crawler/final/predict_label_text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([predict_data_title,pd.DataFrame(preds[:,1],index=predict_data_title.index,columns=['pred'])],axis=1).sort_values('pred',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(text_list, model, tokenizer):  \n",
    "    #tokenize the text\n",
    "    encodings = tokenizer(text_list, \n",
    "                          max_length=15000, \n",
    "                          truncation=True, \n",
    "                          padding=True)\n",
    "    #transform to tf.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "    #predict\n",
    "    preds = model.predict(dataset.batch(1)).logits  \n",
    "    \n",
    "    #transform to array with probabilities\n",
    "    res = tf.nn.softmax(preds, axis=1).numpy()      \n",
    "    \n",
    "    return res\n",
    "\n",
    "predict_proba(strings_list[3], model, tokenizer)[:,1].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
