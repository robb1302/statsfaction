{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer,AutoModelForSequenceClassification,AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"max_columns\", 300)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "os.chdir(os.getcwd().replace('notebooks','').replace('medi_crawler',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Create the data loader with the custom collate function\n",
      "Model Training Loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:19<00:00, 14.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_NAME = \"pritamdeka/PubMedBert-PubMed200kRCT\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 50\n",
    "MAX_LEN = 256\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with features and labels\n",
    "print(\"Load Data\")\n",
    "X = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/labeled_data.csv\")['Title'])\n",
    "y = pd.DataFrame(pd.read_csv(\"data/medi_crawler/processed/labeled_data.csv\")['label'])\n",
    "\n",
    "bool_not_nan = ~X['Title'].isna()\n",
    "X = X[bool_not_nan]\n",
    "y = y[bool_not_nan]\n",
    "y = y.astype(int)\n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert X_train and X_test to list of strings\n",
    "X_train = X_train.squeeze().tolist()\n",
    "X_test = X_test.squeeze().tolist()\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = int(self.labels[idx])  # Convert label to int\n",
    "        \n",
    "        # Tokenize the text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "# Step 2: Dataset Creation\n",
    "dataset = CustomDataset(X_train,  y_train['label'].values)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: DataLoader Setup\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids_batch, attention_mask_batch, labels_batch = zip(*batch)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids_batch = [torch.tensor(ids) for ids in input_ids_batch]\n",
    "    attention_mask_batch = [torch.tensor(mask) for mask in attention_mask_batch]\n",
    "    \n",
    "    # Pad or truncate input sequences to a fixed length\n",
    "    input_ids_batch = pad_sequence(input_ids_batch, batch_first=True, padding_value=0)\n",
    "    attention_mask_batch = pad_sequence(attention_mask_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return input_ids_batch, attention_mask_batch, torch.tensor(labels_batch)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "# Create the data loader with the custom collate function\n",
    "print(\"Create the data loader with the custom collate function\")\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Step 4: Model Training Loop\n",
    "print(\"Model Training Loop\")\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = N_EPOCHS\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# ...\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Calculate recall as a custom loss\n",
    "        recall = recall_score(labels.cpu(), torch.argmax(probabilities, dim=1).cpu(), average='macro', zero_division=0)\n",
    "        recall_loss = 1 - recall  # Use 1 - recall as the loss\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        recall_loss_tensor = torch.tensor(recall_loss, requires_grad=True)  # Convert to a PyTorch tensor\n",
    "        recall_loss_tensor.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print or store the recall value\n",
    "        # print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.40      0.53        35\n",
      "           1       0.30      0.69      0.42        13\n",
      "\n",
      "    accuracy                           0.48        48\n",
      "   macro avg       0.54      0.55      0.47        48\n",
      "weighted avg       0.65      0.48      0.50        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a CustomDataset for the test dataset\n",
    "test_dataset = CustomDataset(X_test, y_test['label'].values)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "model.eval()\n",
    "predicted_scores = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, _ = batch  # Discard labels since they are None\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        scores = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Extract the probability of the positive class (index 1)\n",
    "        positive_scores = scores[:, 1]\n",
    "\n",
    "        predicted_scores.extend(positive_scores.tolist())\n",
    "\n",
    "# Convert the predicted scores to a NumPy array for further analysis\n",
    "predicted_scores = np.round(np.array(predicted_scores),2)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have the predicted labels 'y_pred' and the true labels 'y_true'\n",
    "report = classification_report(y_test, predicted_scores>0)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.25 , 0.001, 0.257, 0.008, 0.08 , 0.016, 0.555, 0.   ,\n",
       "       0.004, 0.001, 0.061, 0.002, 0.019, 0.002, 0.911, 0.001, 0.011,\n",
       "       0.328, 0.007, 0.011, 0.938, 0.02 , 0.   , 0.994, 0.018, 0.   ,\n",
       "       0.014, 0.027, 0.197, 0.   , 0.   , 0.   , 0.005, 0.022, 0.002,\n",
       "       0.687, 0.005, 0.986, 0.004, 0.01 , 0.007, 0.105, 0.003, 0.116,\n",
       "       0.013, 0.481, 0.95 ])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(predicted_scores,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_new_df.squeeze().values\n",
    "\n",
    "encodings = tokenizer(X_new.squeeze().tolist(), \n",
    "                      max_length=MAX_LEN, \n",
    "                      truncation=True, \n",
    "                      padding=True)\n",
    "# Transform to tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(dataset).logits\n",
    "\n",
    "preds = tf.nn.softmax(preds, axis=1).numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([X_new_df,pd.DataFrame(preds[:,1],index=X_new_df.index,columns=['pred'])],axis=1).sort_values('pred',ascending=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('data/medi_crawler/final/predict_label_title_recall.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
