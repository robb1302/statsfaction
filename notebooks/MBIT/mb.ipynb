{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"max_columns\", 300)\n",
    "os.chdir(os.getcwd().replace('notebooks','').replace('MBIT',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 1\n",
    "MAX_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/myer_briggs/raw/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unravelled = pd.DataFrame({\n",
    "    'type': np.repeat(df['type'], df['posts'].str.count('\\|\\|\\|') + 1),\n",
    "    'text': df['posts'].str.split('\\|\\|\\|').explode()\n",
    "})\n",
    "df_unravelled = df_unravelled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>I</th>\n",
       "      <th>N</th>\n",
       "      <th>T</th>\n",
       "      <th>J</th>\n",
       "      <th>E</th>\n",
       "      <th>S</th>\n",
       "      <th>F</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>What has been the most life-changing experience in your life?</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  \\\n",
       "0  INFJ   \n",
       "1  INFJ   \n",
       "2  INFJ   \n",
       "3  INFJ   \n",
       "4  INFJ   \n",
       "\n",
       "                                                                                                                                                      text  \\\n",
       "0                                                                                                              'http://www.youtube.com/watch?v=qsXHcwe3krw   \n",
       "1                                                                                            http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg   \n",
       "2  enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks   \n",
       "3                                                                                            What has been the most life-changing experience in your life?   \n",
       "4                                    http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.   \n",
       "\n",
       "      I     N      T     J      E      S     F      P  \n",
       "0  True  True  False  True  False  False  True  False  \n",
       "1  True  True  False  True  False  False  True  False  \n",
       "2  True  True  False  True  False  False  True  False  \n",
       "3  True  True  False  True  False  False  True  False  \n",
       "4  True  True  False  True  False  False  True  False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unravelled['I'] = ['I' in c for c in df_unravelled.type]\n",
    "df_unravelled['N'] = ['N' in c for c in df_unravelled.type]\n",
    "df_unravelled['T'] = ['T' in c for c in df_unravelled.type]\n",
    "df_unravelled['J'] = ['J' in c for c in df_unravelled.type]\n",
    "df_unravelled['E'] = ['E' in c for c in df_unravelled.type]\n",
    "df_unravelled['S'] = ['S' in c for c in df_unravelled.type]\n",
    "df_unravelled['F'] = ['F' in c for c in df_unravelled.type]\n",
    "df_unravelled['P'] = ['P' in c for c in df_unravelled.type]\n",
    "df_unravelled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_unravelled.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sample['text']\n",
    "y = df_sample['I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.783333\n",
       "False    0.216667\n",
       "Name: I, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.773571\n",
       "False    0.226429\n",
       "Name: I, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a tokenizer object\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "#tokenize the text\n",
    "train_encodings = tokenizer(list(X_train.values),\n",
    "                            truncation=True, \n",
    "                            padding=True)\n",
    "test_encodings = tokenizer(list(X_test.values),\n",
    "                           truncation=True, \n",
    "                           padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n",
    "                                    list(y_train.values)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n",
    "                                    list(y_test.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics.keras import BinaryF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 538s 23s/step - loss: 0.8699 - binary_f1_score: 0.8723\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\keras\\engine\\training.py:2416: UserWarning: Metric BinaryF1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/22 [===========================>..] - ETA: 40s - loss: 0.5328 - binary_f1_score: 0.8739 "
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "#chose the optimizer\n",
    "optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "#define the loss function \n",
    "losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "#build the model\n",
    "model.compile(optimizer=optimizerr,\n",
    "              loss=losss,\n",
    "              metrics=[BinaryF1Score()])\n",
    "# train the model \n",
    "model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),\n",
    "          epochs=N_EPOCHS,\n",
    "          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Robert\\Documents\\Projekte\\dev\\statsfaction\\.venv\\lib\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...classifier\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\n",
      "......vars\n",
      "...distilbert\\embeddings\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\embeddings\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\embeddings\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_1\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_2\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_3\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_4\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\k_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\out_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\q_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\attention\\v_lin\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\dropout\n",
      "......vars\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\lin1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\ffn\\lin2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\output_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...distilbert\\transformer\\layer\\tf_transformer_block_5\\sa_layer_norm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...dropout\n",
      "......vars\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\binary_f1_score\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........100\n",
      ".........101\n",
      ".........102\n",
      ".........103\n",
      ".........104\n",
      ".........105\n",
      ".........106\n",
      ".........107\n",
      ".........108\n",
      ".........109\n",
      ".........11\n",
      ".........110\n",
      ".........111\n",
      ".........112\n",
      ".........113\n",
      ".........114\n",
      ".........115\n",
      ".........116\n",
      ".........117\n",
      ".........118\n",
      ".........119\n",
      ".........12\n",
      ".........120\n",
      ".........121\n",
      ".........122\n",
      ".........123\n",
      ".........124\n",
      ".........125\n",
      ".........126\n",
      ".........127\n",
      ".........128\n",
      ".........129\n",
      ".........13\n",
      ".........130\n",
      ".........131\n",
      ".........132\n",
      ".........133\n",
      ".........134\n",
      ".........135\n",
      ".........136\n",
      ".........137\n",
      ".........138\n",
      ".........139\n",
      ".........14\n",
      ".........140\n",
      ".........141\n",
      ".........142\n",
      ".........143\n",
      ".........144\n",
      ".........145\n",
      ".........146\n",
      ".........147\n",
      ".........148\n",
      ".........149\n",
      ".........15\n",
      ".........150\n",
      ".........151\n",
      ".........152\n",
      ".........153\n",
      ".........154\n",
      ".........155\n",
      ".........156\n",
      ".........157\n",
      ".........158\n",
      ".........159\n",
      ".........16\n",
      ".........160\n",
      ".........161\n",
      ".........162\n",
      ".........163\n",
      ".........164\n",
      ".........165\n",
      ".........166\n",
      ".........167\n",
      ".........168\n",
      ".........169\n",
      ".........17\n",
      ".........170\n",
      ".........171\n",
      ".........172\n",
      ".........173\n",
      ".........174\n",
      ".........175\n",
      ".........176\n",
      ".........177\n",
      ".........178\n",
      ".........179\n",
      ".........18\n",
      ".........180\n",
      ".........181\n",
      ".........182\n",
      ".........183\n",
      ".........184\n",
      ".........185\n",
      ".........186\n",
      ".........187\n",
      ".........188\n",
      ".........189\n",
      ".........19\n",
      ".........190\n",
      ".........191\n",
      ".........192\n",
      ".........193\n",
      ".........194\n",
      ".........195\n",
      ".........196\n",
      ".........197\n",
      ".........198\n",
      ".........199\n",
      ".........2\n",
      ".........20\n",
      ".........200\n",
      ".........201\n",
      ".........202\n",
      ".........203\n",
      ".........204\n",
      ".........205\n",
      ".........206\n",
      ".........207\n",
      ".........208\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........29\n",
      ".........3\n",
      ".........30\n",
      ".........31\n",
      ".........32\n",
      ".........33\n",
      ".........34\n",
      ".........35\n",
      ".........36\n",
      ".........37\n",
      ".........38\n",
      ".........39\n",
      ".........4\n",
      ".........40\n",
      ".........41\n",
      ".........42\n",
      ".........43\n",
      ".........44\n",
      ".........45\n",
      ".........46\n",
      ".........47\n",
      ".........48\n",
      ".........49\n",
      ".........5\n",
      ".........50\n",
      ".........51\n",
      ".........52\n",
      ".........53\n",
      ".........54\n",
      ".........55\n",
      ".........56\n",
      ".........57\n",
      ".........58\n",
      ".........59\n",
      ".........6\n",
      ".........60\n",
      ".........61\n",
      ".........62\n",
      ".........63\n",
      ".........64\n",
      ".........65\n",
      ".........66\n",
      ".........67\n",
      ".........68\n",
      ".........69\n",
      ".........7\n",
      ".........70\n",
      ".........71\n",
      ".........72\n",
      ".........73\n",
      ".........74\n",
      ".........75\n",
      ".........76\n",
      ".........77\n",
      ".........78\n",
      ".........79\n",
      ".........8\n",
      ".........80\n",
      ".........81\n",
      ".........82\n",
      ".........83\n",
      ".........84\n",
      ".........85\n",
      ".........86\n",
      ".........87\n",
      ".........88\n",
      ".........89\n",
      ".........9\n",
      ".........90\n",
      ".........91\n",
      ".........92\n",
      ".........93\n",
      ".........94\n",
      ".........95\n",
      ".........96\n",
      ".........97\n",
      ".........98\n",
      ".........99\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-07-01 17:08:14         2053\n",
      "metadata.json                                  2023-07-01 17:08:14           64\n",
      "variables.h5                                   2023-07-01 17:08:17    803776944\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pickle\n",
    "\n",
    "# Assuming you have a trained model object called 'model'\n",
    "# Save the model as a pickle file\n",
    "with open('data/myer_briggs/model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(text_list, model, tokenizer):  \n",
    "    #tokenize the text\n",
    "    encodings = tokenizer(text_list, \n",
    "                          max_length=15000, \n",
    "                          truncation=True, \n",
    "                          padding=True)\n",
    "    #transform to tf.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))\n",
    "    #predict\n",
    "    preds = model.predict(dataset.batch(1)).logits  \n",
    "    \n",
    "    #transform to array with probabilities\n",
    "    res = tf.nn.softmax(preds, axis=1).numpy()      \n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introvert True\n",
      "50/50 [==============================] - 1s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77337915"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 90\n",
    "print('Introvert',y_test.iloc[i])\n",
    "\n",
    "predict_proba(X_test.iloc[i], model, tokenizer)[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 85s 140ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_dataset).logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.21      0.42      0.28       130\n",
      "        True       0.78      0.56      0.65       470\n",
      "\n",
      "    accuracy                           0.53       600\n",
      "   macro avg       0.49      0.49      0.46       600\n",
      "weighted avg       0.65      0.53      0.57       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_score = np.abs(y_pred[:,1])>0.5\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_score)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
