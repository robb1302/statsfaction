{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Players with low potentials but high actual Rating\n",
    "- Label Players has a potential higher than 83 but never reaches this potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERTRAINING = True\n",
    "CV = 5\n",
    "PLAYER_ATTRIBUTES = [ 'central','winger','offense','Finishing',  'ShortPassing', 'Volleys', 'Dribbling',  'FKAccuracy', 'LongPassing', 'BallControl',\n",
    "                      'Acceleration', 'SprintSpeed', 'Agility',    'Reactions', 'Balance', \n",
    "                      'ShotPower', 'Jumping',  'LongShots', 'Positioning', 'Vision' ]\n",
    "# PLAYER_ATTRIBUTES = [ 'Age' ,'Dribbling',  'FKAccuracy',  'BallControl','ShotPower','Positioning', 'Penalties' ]\n",
    "PLAYER_ATTRIBUTES = [  'central','offense','Age','Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n",
    "                      'Acceleration', 'SprintSpeed', 'Agility', 'GKPositioning', 'GKReflexes', 'Composure', 'Defensive awareness', 'Reactions', 'Balance', \n",
    "                      'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Marking', \n",
    "                      'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking']\n",
    "PLAYER_ATTRIBUTES = ['Reactions', 'age_based_Stamina', 'Positioning', 'FKAccuracy','ShortPassing',  'Dribbling', 'BallControl', 'ShotPower',    'Aggression',   'Vision',  'SprintSpeed']\n",
    "# PLAYER_ATTRIBUTES = [ 'Acceleration', 'SprintSpeed', 'Agility',    'Reactions', 'Balance',  'Jumping', 'Stamina', 'Strength',  'Aggression', 'Positioning', 'Vision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def find_and_append_module_path():\n",
    "    current_dir = os.getcwd()\n",
    "    substring_to_find = 'statsfaction'\n",
    "    index = current_dir.rfind(substring_to_find)\n",
    "    \n",
    "    if index != -1:\n",
    "        # Extract the directory path up to and including the last \"mypath\" occurrence\n",
    "        new_dir = current_dir[:index + (len(substring_to_find))]\n",
    "\n",
    "        # Change the current working directory to the new directory\n",
    "        os.chdir(new_dir)\n",
    "        sys.path.append(new_dir)\n",
    "        # Verify the new current directory\n",
    "        print(\"New current directory:\", os.getcwd())\n",
    "    else:\n",
    "        print(\"No 'mypath' found in the current directory\")\n",
    "find_and_append_module_path()\n",
    "os.getcwd()\n",
    "\n",
    "from src.sport_analytics.model.prepare import add_features_raw_datadf_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prime = \"\"\"\n",
    "SELECT max(Age) as prime_age,* FROM(SELECT MAX(Overall) AS PrimeOverall,*\n",
    "  FROM fifa\n",
    "  GROUP BY ID ) \n",
    "  GROUP BY ID\n",
    "  order by PrimeOverall DESC;\n",
    "\"\"\"\n",
    "\n",
    "sql_potentials = f\"\"\"\n",
    "SELECT min(Age) as potential_age,* FROM  (SELECT *,Potential as max_potential FROM fifa WHERE Potential>84)\n",
    "GROUP BY ID\n",
    "order by potential DESC;\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "DATABASE_PATH = \"data/sport_analytics/database/football.db\"\n",
    "# Step 1: Establish a database connection\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "df_potentials = pd.read_sql_query(sql_potentials, conn)\n",
    "df_prime = pd.read_sql_query(sql_prime, conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df_potentials = df_potentials.set_index(['ID'])\n",
    "df_prime = df_prime.set_index(['ID'])\n",
    "\n",
    "df_raw = df_potentials.join(df_prime[[\"prime_age\",\"PrimeOverall\"]])\n",
    "df_raw = df_raw.reset_index(['ID'])\n",
    "df_raw = add_features_raw_datadf_raw(df_raw)\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "year_to_category = {2011: 'drop', 2012: 'train', 2013: 'train', 2014: 'train', 2015: 'train', 2016: 'train', 2017: 'train', 2018: 'train', 2019: 'test', 2020: 'test', 2021: 'test', 2022: 'valid', 2023: 'valid', 2024: 'valid'}\n",
    "df['set'] = df.index.get_level_values('FIFA').values\n",
    "# Apply the mapping to the \"FIFA\" column\n",
    "df['set'] = df['set'].map(year_to_category)\n",
    "\n",
    "df_potentials = df[(df.set==\"valid\")&(df.Overall<83)&(df.Age<26)&(df.Potential>83)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df.prime_age>df.potential_age]\n",
    "df['target'] = df.PrimeOverall>82\n",
    "df = df[df.potential_age<26]\n",
    "df = df[df.offense>0.5]\n",
    "print(df.target.value_counts())\n",
    "PREDICTION_NAME = \"Offense\"\n",
    "\n",
    "df_processed = df.copy()\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.best_position.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"potential_age\",\"Age\",\"prime_age\",\"max_potential\",\"Potential\",\"Overall\",\"PrimeOverall\",\"target\",\"set\",\"best_position\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     df = df[[any(pos in i for pos in ['CF', 'LW', 'ST', 'RW']) for i in df['Position']]]\n",
    "#     df.shape\n",
    "# else:\n",
    "#     select_position = lambda x: x in [\"ST\",\"CF\",\"LW\",\"RW\"]\n",
    "#     df[\"select\"] = df['Position'].apply(select_position)\n",
    "#     df = df[df[\"select\"]]\n",
    "#     df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deskriptive Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude='object').corr()['target'].round(2).sort_values().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Age<20].describe().round(0).compare(df[df.Age>=20].describe().round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.target].describe().round(3)\n",
    "pd.set_option('display.max_columns', 3000)\n",
    "bool_age = df.Age == 20\n",
    "df[df.target&bool_age].describe().round(3).compare(df[~df.target&bool_age].describe().round(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Defense Attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_processed.fillna(0)\n",
    "df_potentials = df_potentials.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop(\"target\", axis=1, errors='ignore')\n",
    "y = df['target']  # Use df_processed here instead of df\n",
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "if False:\n",
    "    # Step 1: Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = X_train[PLAYER_ATTRIBUTES]\n",
    "    X_test = X_test[PLAYER_ATTRIBUTES]\n",
    "\n",
    "else:\n",
    "\n",
    "    X_train = X[X.set==\"train\"][PLAYER_ATTRIBUTES]\n",
    "    y_train = y[X.set==\"train\"]\n",
    "\n",
    "    X_test = X[X.set==\"test\"][PLAYER_ATTRIBUTES]\n",
    "    y_test = y[X.set==\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load your dataset or replace df_processed and df with your data\n",
    "# df_processed = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Step 2: Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 3: Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df_potentials_scaled = scaler.transform(df_potentials[PLAYER_ATTRIBUTES].fillna(0))\n",
    "\n",
    "# Step 4: Create new DataFrames with the scaled data while preserving the index and columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=PLAYER_ATTRIBUTES)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=PLAYER_ATTRIBUTES)\n",
    "df_potentials_scaled_df = pd.DataFrame(df_potentials_scaled, index=df_potentials.index, columns=PLAYER_ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 8),\n",
    "            'min_samples_split': trial.suggest_float('min_samples_split', 0.1, 1.0),  # Adjust the range\n",
    "            'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.1, 0.5),  # Adjust the range\n",
    "            'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "            'criterion': 'gini',  # or 'entropy' depending on your problem\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Implement cross-validation\n",
    "        cv_scores = cross_val_score(RandomForestClassifier(**params), X_train_scaled_df, y_train, cv=CV, scoring='roc_auc')\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAMS_RF = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAMS_RF)\n",
    "    print(\"Best AUC:\", best_auc)\n",
    "else:\n",
    "    PARAMS_RF = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 12),\n",
    "            'min_samples_split': trial.suggest_float('min_samples_split', 0.1, 1.0),  # Adjust the range\n",
    "            'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.1, 0.5),  # Adjust the range\n",
    "            'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "            'criterion': 'gini',  # or 'entropy' depending on your problem\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Create the Decision Tree classifier with the given hyperparameters\n",
    "        clf = DecisionTreeClassifier(**params)\n",
    "\n",
    "        # Implement cross-validation to calculate mean AUC\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled_df, y_train, cv=CV, scoring='roc_auc')\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAM_DT = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAM_DT)\n",
    "    print(\"Best AUC:\", best_auc)\n",
    "else:\n",
    "    PARAM_DT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 400),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 8),\n",
    "            'min_child_weight': trial.suggest_float('min_child_weight', 5, 20),  # Adjust the range\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),\n",
    "        }\n",
    "\n",
    "        # Implement early stopping with cross-validation\n",
    "        cv_scores = []\n",
    "\n",
    "        clf = XGBClassifier(**params, random_state=42, n_jobs=-1)\n",
    "        # Implement cross-validation to calculate mean AUC\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled_df, y_train, cv=CV, scoring='roc_auc')\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAM_XGB = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAM_XGB)\n",
    "    print(\"Best AUC:\", best_auc)\n",
    "else:\n",
    "    PARAM_XGB = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL Model RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import shap \n",
    "import shap  # For SHAP values\n",
    "import pickle\n",
    "\n",
    "# Define class weights (if applicable)\n",
    "class_weights = 'balanced'  # You can customize these weights if needed\n",
    "\n",
    "# Create and train different classification models with class weights\n",
    "models = {\n",
    "    # 'Logistic Regression': LogisticRegression(class_weight=class_weights),\n",
    "    'Decision Tree Classifier': DecisionTreeClassifier(class_weight=class_weights,** PARAM_DT),\n",
    "    'Random Forest Classifier': RandomForestClassifier(class_weight=class_weights, ** PARAMS_RF),\n",
    "    'XGBoost Classifier general': xgb.XGBClassifier(),\n",
    "                                                                                     \n",
    "    # 'Support Vector Classifier': SVC(class_weight=class_weights,probability=True,kernel='linear'),\n",
    "    'XGBoost Classifier': xgb.XGBClassifier(**PARAM_XGB)\n",
    "    # 'LightGBM Classifier': lgb.LGBMClassifier(** {'n_estimators': 100, 'max_depth': 7, 'min_child_samples': 7, 'subsample': 0.8090291881142198, 'colsample_bytree': 0.6278496311554949, 'learning_rate': 0.17976777947590364, 'reg_alpha': 0.6699371890412207, 'reg_lambda': 1.5295937299896694})\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(model_name,\"training----->\")\n",
    "    model.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled_df)\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "    else:\n",
    "        y_prob = None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    if y_prob is not None:\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        auc = None\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'Model': model,\n",
    "        'Scaler':scaler,\n",
    "        'attributes':PLAYER_ATTRIBUTES,\n",
    "        'Accuracy': accuracy,\n",
    "        'Classification Report': report,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'AUC': auc,\n",
    "        'Precision': precision\n",
    "    }\n",
    "if True:\n",
    "    try:\n",
    "        # Save SHAP values to a pickle file\n",
    "        with open(f\"data/sport_analytics/model/{PREDICTION_NAME}_trained_models.pkl\", \"wb\") as file:\n",
    "            pickle.dump(model_results, file)\n",
    "    except:\n",
    "        print(\"Fehler save Models\",model_name)\n",
    "\n",
    "# Evaluate and print results for each model\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(results['Classification Report'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(results['Confusion Matrix'])\n",
    "    if results['AUC'] is not None:\n",
    "        print(f\"AUC: {results['AUC']:.2f}\")\n",
    "    if 'Precision' in results:\n",
    "        print(f\"Precision: {results['Precision']:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = model_results['XGBoost Classifier']['Model']\n",
    "\n",
    "\n",
    "from src.sport_analytics.model.eval import individual_shap_valuess,plot_feature_importance\n",
    "\n",
    "features = plot_feature_importance(my_model, '')\n",
    "explainer = shap.Explainer(my_model)\n",
    "shap_values = explainer.shap_values(X_test_scaled_df)\n",
    "shap.summary_plot(shap_values, X_test_scaled_df)\n",
    "\n",
    "features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = (my_model.predict_proba(X_test_scaled_df)[:,1]>0.5)&(~y_test)\n",
    "X_test_scaled_df[false_positives]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = (my_model.predict_proba(X_test_scaled_df)[:,1]<0.4)&(y_test)\n",
    "X_test_scaled_df[false_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
