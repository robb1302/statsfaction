{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Players with low potentials but high actual Rating\n",
    "- Label Players has a potential higher than 83 but never reaches this potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERTRAINING = False\n",
    "CV = 5\n",
    "SCORING = 'recall'\n",
    "CLASS_WEIGHTS = 'balanced'\n",
    "EXPERIEMENT_NAME = \"offense_potential_predictor\"\n",
    "RUN_NAME = None\n",
    "TARGET_OVERALL = 80\n",
    "\n",
    "PLAYER_ATTRIBUTES = [ 'central','winger','offense','Finishing',  'ShortPassing', 'Volleys', 'Dribbling',  'FKAccuracy', 'LongPassing', 'BallControl',\n",
    "                      'Acceleration', 'SprintSpeed', 'Agility',    'Reactions', 'Balance', \n",
    "                      'ShotPower', 'Jumping',  'LongShots', 'Positioning', 'Vision' ]\n",
    "# PLAYER_ATTRIBUTES = [ 'Age' ,'Dribbling',  'FKAccuracy',  'BallControl','ShotPower','Positioning', 'Penalties' ]\n",
    "PLAYER_ATTRIBUTES = [  'central','offense','Age','Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n",
    "                      'Acceleration', 'SprintSpeed', 'Agility', 'GKPositioning', 'GKReflexes', 'Composure', 'Defensive awareness', 'Reactions', 'Balance', \n",
    "                      'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Marking', \n",
    "                      'StandingTackleshooting_technique', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking']\n",
    "PLAYER_ATTRIBUTES = ['Reactions', 'age_based_Stamina', 'Positioning', 'ShortPassing',  'Dribbling', 'BallControl',    'Aggression',   'Vision',  'SprintSpeed','shooting']\n",
    "PLAYER_ATTRIBUTES = ['Crossing', 'Finishing','shooting_technique','mental'\n",
    "       'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy',\n",
    "       'LongPassing', 'BallControl',  'SprintSpeed', 'Agility',\n",
    "       'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength',\n",
    "       'LongShots', 'Aggression',  'Positioning', 'Vision',\n",
    "       'Penalties',  \n",
    "        'youth_player', 'shooting', '',\n",
    "       'mental', 'physique', 'Speed', 'ball_handling', 'age_based_Reactions',\n",
    "       'age_based_physique', 'age_based_shooting_technique',\n",
    "       'age_based_Stamina', 'age_based_Positioning', 'age_based_Vision',\n",
    "       'age_based_Finishing', 'age_based_BallControl']\n",
    "\n",
    "PLAYER_ATTRIBUTES = [ 'age_based_shooting_technique', 'Crossing',  'HeadingAccuracy', 'ShortPassing','Dribbling',    'BallControl',\n",
    "                     'SprintSpeed', 'Agility','Composure', 'Reactions', \n",
    "                       'Stamina', 'Aggression', 'Interceptions', 'Positioning', 'Vision']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def find_and_append_module_path():\n",
    "    current_dir = os.getcwd()\n",
    "    substring_to_find = 'statsfaction'\n",
    "    index = current_dir.rfind(substring_to_find)\n",
    "    \n",
    "    if index != -1:\n",
    "        # Extract the directory path up to and including the last \"mypath\" occurrence\n",
    "        new_dir = current_dir[:index + (len(substring_to_find))]\n",
    "\n",
    "        # Change the current working directory to the new directory\n",
    "        os.chdir(new_dir)\n",
    "        sys.path.append(new_dir)\n",
    "        # Verify the new current directory\n",
    "        print(\"New current directory:\", os.getcwd())\n",
    "    else:\n",
    "        print(\"No 'mypath' found in the current directory\")\n",
    "find_and_append_module_path()\n",
    "os.getcwd()\n",
    "\n",
    "from src.sport_analytics.model.prepare import add_features_raw_datadf_raw\n",
    "from src.sport_analytics.model.eval import plot_feature_importance,plot_shap_summary,plot_auc_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prime = \"\"\"\n",
    "SELECT max(Age) as prime_age,* FROM(SELECT MAX(Overall) AS PrimeOverall,*\n",
    "  FROM fifa\n",
    "  GROUP BY ID ) \n",
    "  GROUP BY ID\n",
    "  order by PrimeOverall DESC;\n",
    "\"\"\"\n",
    "\n",
    "sql_potentials = f\"\"\"\n",
    "SELECT min(Age) as potential_age,* FROM  (SELECT *,Potential as max_potential FROM fifa WHERE Potential>={TARGET_OVERALL})\n",
    "GROUP BY ID\n",
    "order by potential DESC;\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "DATABASE_PATH = \"data/sport_analytics/database/football.db\"\n",
    "# Step 1: Establish a database connection\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "df_potentials = pd.read_sql_query(sql_potentials, conn)\n",
    "df_prime = pd.read_sql_query(sql_prime, conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df_potentials = df_potentials.set_index(['ID'])\n",
    "df_prime = df_prime.set_index(['ID'])\n",
    "\n",
    "df_raw = df_potentials.join(df_prime[[\"prime_age\",\"PrimeOverall\"]])\n",
    "df_raw = df_raw.reset_index(['ID'])\n",
    "df_raw = add_features_raw_datadf_raw(df_raw)\n",
    "\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "year_to_category = {2011: 'drop', 2012: 'train', 2013: 'train', 2014: 'train', 2015: 'train', 2016: 'train', 2017: 'train', 2018: 'train', 2019: 'test', 2020: 'test', 2021: 'test', 2022: 'test', 2023: 'valid', 2024: 'valid'}\n",
    "df['set'] = df.index.get_level_values('FIFA').values\n",
    "# Apply the mapping to the \"FIFA\" column\n",
    "df['set'] = df['set'].map(year_to_category)\n",
    "\n",
    "df_potentials = df[(df.set==\"valid\")&(df.Overall<TARGET_OVERALL)&(df.Age<26)&(df.Potential>=TARGET_OVERALL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df.prime_age>df.potential_age]\n",
    "df['target'] = df.PrimeOverall>=TARGET_OVERALL\n",
    "df = df[df.potential_age<26]\n",
    "df = df[df.offense>0.5]\n",
    "print(df.target.value_counts())\n",
    "PREDICTION_NAME = \"Offense\"\n",
    "\n",
    "df_processed = df.copy()\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.best_position.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"potential_age\",\"Age\",\"prime_age\",\"max_potential\",\"Potential\",\"Overall\",\"PrimeOverall\",\"target\",\"set\",\"best_position\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     df = df[[any(pos in i for pos in ['CF', 'LW', 'ST', 'RW']) for i in df['Position']]]\n",
    "#     df.shape\n",
    "# else:\n",
    "#     select_position = lambda x: x in [\"ST\",\"CF\",\"LW\",\"RW\"]\n",
    "#     df[\"select\"] = df['Position'].apply(select_position)\n",
    "#     df = df[df[\"select\"]]\n",
    "#     df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deskriptive Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude='object').corr()['target'].round(2).sort_values().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Age<20].describe().round(0).compare(df[df.Age>=20].describe().round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.target].describe().round(3)\n",
    "pd.set_option('display.max_columns', 3000)\n",
    "bool_age = df.Age == 20\n",
    "df[df.target&bool_age].describe().round(3).compare(df[~df.target&bool_age].describe().round(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_processed.fillna(0)\n",
    "df_potentials = df_potentials.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop(\"target\", axis=1, errors='ignore')\n",
    "y = df['target']  # Use df_processed here instead of df\n",
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "if False:\n",
    "    # Step 1: Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = X_train[PLAYER_ATTRIBUTES]\n",
    "    X_test = X_test[PLAYER_ATTRIBUTES]\n",
    "\n",
    "else:\n",
    "\n",
    "    X_train = X[X.set==\"train\"][PLAYER_ATTRIBUTES]\n",
    "    y_train = y[X.set==\"train\"]\n",
    "\n",
    "    X_test = X[X.set==\"test\"][PLAYER_ATTRIBUTES]\n",
    "    y_test = y[X.set==\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~y_train).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load your dataset or replace df_processed and df with your data\n",
    "# df_processed = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Step 2: Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 3: Fit the scaler on the training data and transform both training \n",
    "# and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df_potentials_scaled = scaler.transform(df_potentials[PLAYER_ATTRIBUTES].fillna(0))\n",
    "\n",
    "# Step 4: Create new DataFrames with the scaled data while preserving the index and columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=PLAYER_ATTRIBUTES)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=PLAYER_ATTRIBUTES)\n",
    "df_potentials_scaled_df = pd.DataFrame(df_potentials_scaled, index=df_potentials.index, columns=PLAYER_ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 8),\n",
    "            'min_samples_split': trial.suggest_float('min_samples_split', 0.1, 1.0),  # Adjust the range\n",
    "            'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.1, 0.5),  # Adjust the range\n",
    "            'max_features': trial.suggest_float('max_features', 0.6, 1.0),\n",
    "            'criterion': 'entropy',  # or 'entropy' depending on your problem\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Implement cross-validation\n",
    "        cv_scores = cross_val_score(RandomForestClassifier(**params), X_train_scaled_df, y_train, cv=CV, scoring=SCORING)\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAMS_RF = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAMS_RF)\n",
    "    print(f\"Best {SCORING}:\", best_auc)\n",
    "else:\n",
    "    PARAMS_RF = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 12),\n",
    "            'min_samples_split': trial.suggest_float('min_samples_split', 0.1, 1.0),  # Adjust the range\n",
    "            'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.1, 0.5),  # Adjust the range\n",
    "            'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "            'criterion': 'gini',  # or 'entropy' depending on your problem\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Create the Decision Tree classifier with the given hyperparameters\n",
    "        clf = DecisionTreeClassifier(**params)\n",
    "\n",
    "        # Implement cross-validation to calculate mean AUC\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled_df, y_train, cv=CV, scoring='recall_macro')\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAM_DT = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAM_DT)\n",
    "    print(f\"Best {SCORING}:\", best_auc)\n",
    "else:\n",
    "    PARAM_DT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.3),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'min_child_weight': trial.suggest_uniform('min_child_weight', 1.0, 20.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.7, 1.0),\n",
    "            'reg_alpha': trial.suggest_uniform('reg_alpha', 0.1, 1.0),\n",
    "            'reg_lambda': trial.suggest_uniform('reg_lambda', 0.01, 0.1),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        }\n",
    "        # Implement early stopping with cross-validation\n",
    "        cv_scores = []\n",
    "\n",
    "        clf = XGBClassifier(**params, random_state=42, n_jobs=-1)\n",
    "        # Implement cross-validation to calculate mean AUC\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled_df, y_train, cv=CV, scoring=SCORING)\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAM_XGB = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAM_XGB)\n",
    "    print(f\"Best {SCORING}:\", best_auc)\n",
    "else:\n",
    "    PARAM_XGB = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "- f1 Score für false Predictions -> DONE\n",
    "- save shap values für alle Modelle -> DONE\n",
    "- plots in einem Ordner saven -> DONE\n",
    "- Random State hinzufügen -> DONE\n",
    "- Vereinfachung des Codes -> DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "from src.sport_analytics.model.eval import plot_feature_importance, plot_auc_curves,plot_shap_summary,log_metrics_in_mlflow,log_feature_list_as_artifact\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Create and train different classification models with class weights\n",
    "models = {\n",
    "    # 'Logistic Regression': LogisticRegression(class_weight=class_weights),\n",
    "    'Decision Tree Classifier': DecisionTreeClassifier(random_state=42,class_weight=CLASS_WEIGHTS, **PARAM_DT),\n",
    "    'Random Forest Classifier': RandomForestClassifier(random_state=42,class_weight=CLASS_WEIGHTS, **PARAMS_RF),\n",
    "    'XGBoost Classifier': xgb.XGBClassifier(random_state=42,**PARAM_XGB),\n",
    "    # 'Support Vector Classifier': SVC(class_weight=class_weights, probability=True, kernel='linear'),\n",
    "\n",
    "    'LightGBM Classifier': lgb.LGBMClassifier(random_state=42,**{'n_estimators': 100, 'max_depth': 7, 'min_child_samples': 7,\n",
    "                                                  'subsample': 0.8090291881142198, 'colsample_bytree': 0.6278496311554949,\n",
    "                                                  'learning_rate': 0.17976777947590364, 'reg_alpha': 0.6699371890412207,\n",
    "                                                  'reg_lambda': 1.5295937299896694})\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(EXPERIEMENT_NAME)\n",
    "\n",
    "# Start MLflow run with a specific run name and description\n",
    "for model_name, model in models.items():\n",
    "    with mlflow.start_run(run_name=RUN_NAME):\n",
    "\n",
    "        print(model_name, \"training----->\")\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"Model_Name\", model_name)\n",
    "        mlflow.log_params(model.get_params())\n",
    "\n",
    "        model.fit(X_train_scaled_df, y_train)\n",
    "        y_pred = model.predict(X_test_scaled_df)\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_prob = model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "        else:\n",
    "            y_prob = None\n",
    "        \n",
    "        # Log Params\n",
    "        log_feature_list_as_artifact(PLAYER_ATTRIBUTES, filename=\"feature_list.txt\")\n",
    "        \n",
    "        # Create a dictionary with parameters and their values\n",
    "        params_to_log = {\n",
    "            'CLASS_WEIGHTS': CLASS_WEIGHTS,\n",
    "            'HYPERTRAINING': HYPERTRAINING,\n",
    "            'CV': CV,\n",
    "            'SCORING': SCORING,\n",
    "            'features_anzahl': len(PLAYER_ATTRIBUTES),\n",
    "            'y_train_positives': y_train.sum(),\n",
    "            'y_train_negatives': (~y_train).sum(),\n",
    "            'y_test_positives': y_test.sum(),\n",
    "            'y_test_negatives': (~y_test).sum(),\n",
    "            'TARGET_OVERALL':TARGET_OVERALL\n",
    "        }\n",
    "\n",
    "        # Log parameters using log_params\n",
    "        # mlflow.log_params(params_to_log)\n",
    "\n",
    "\n",
    "        # # Log artifacts\n",
    "        # mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "        # Evaluation Plots\n",
    "        plot_auc_curves(y_true = y_test,y_proba = model.predict_proba(X_test_scaled_df))\n",
    "        plot_feature_importance(model, '',top_n=20)\n",
    "        plot_shap_summary(model=model,df=X_test_scaled_df)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        log_metrics_in_mlflow(y_test=y_test,y_prob=y_prob,y_pred=y_pred)\n",
    "\n",
    "\n",
    "        # Output for quick evaluation\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        precision_pos, recall_pos, f1_pos, _ = precision_recall_fscore_support(y_test, y_pred, labels=[1], average='binary')\n",
    "\n",
    "        if y_prob is not None:\n",
    "                roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        else:\n",
    "            roc_auc = None\n",
    "        \n",
    "        \n",
    "        model_results[model_name] = {\n",
    "            'Model': model,\n",
    "            'Scaler': scaler,\n",
    "            'attributes': PLAYER_ATTRIBUTES,\n",
    "            'Accuracy': accuracy,\n",
    "            'Classification Report': report,\n",
    "            'Confusion Matrix': conf_matrix,\n",
    "            'AUC': roc_auc,\n",
    "            'recall':recall_pos,\n",
    "            'f1':f1_pos,\n",
    "            'Precision': precision_pos\n",
    "        }\n",
    "\n",
    "\n",
    "# Evaluate and print results for each model\n",
    "for model_name, results in model_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.2f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(results['Classification Report'])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(results['Confusion Matrix'])\n",
    "    if results['AUC'] is not None:\n",
    "        print(f\"AUC: {results['AUC']:.2f}\")\n",
    "    if 'Precision' in results:\n",
    "        print(f\"Precision: {results['Precision']:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = model_results['Random Forest Classifier']['Model']\n",
    "\n",
    "false_positives = (my_model.predict_proba(X_test_scaled_df)[:,1]>0.5)&(~y_test)\n",
    "X_test_scaled_df[false_positives]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = (my_model.predict_proba(X_test_scaled_df)[:,1]<0.3)&(y_test)\n",
    "X_test_scaled_df[false_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
