{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Players with low potentials but high actual Rating\n",
    "- Label Players has a potential higher than 83 but never reaches this potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERTRAINING = False\n",
    "CV = 5\n",
    "SCORING = ''\n",
    "SAVE_MODEL_NAME = 'offense_regressor'\n",
    "CLASS_WEIGHTS = 'balanced'\n",
    "EXPERIEMENT_NAME = \"offense_potential_predictor_regression\"\n",
    "RUN_NAME = None\n",
    "TARGET_OVERALL = 80\n",
    "\n",
    "PLAYER_ATTRIBUTES = [ 'central','winger','offense','Finishing',  'ShortPassing', 'Volleys', 'Dribbling',  'FKAccuracy', 'LongPassing', 'BallControl',\n",
    "                      'Acceleration', 'SprintSpeed', 'Agility',    'Reactions', 'Balance', \n",
    "                      'ShotPower', 'Jumping',  'LongShots', 'Positioning', 'Vision' ]\n",
    "# PLAYER_ATTRIBUTES = [ 'Age' ,'Dribbling',  'FKAccuracy',  'BallControl','ShotPower','Positioning', 'Penalties' ]\n",
    "PLAYER_ATTRIBUTES = [  'central','offense','Age','Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl',\n",
    "                      'Acceleration', 'SprintSpeed', 'Agility', 'GKPositioning', 'GKReflexes', 'Composure', 'Defensive awareness', 'Reactions', 'Balance', \n",
    "                      'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Marking', \n",
    "                      'StandingTackleshooting_technique', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking']\n",
    "PLAYER_ATTRIBUTES = ['Reactions', 'age_based_Stamina', 'Positioning', 'ShortPassing',  'Dribbling', 'BallControl',    'Aggression',   'Vision',  'SprintSpeed','shooting']\n",
    "PLAYER_ATTRIBUTES = ['Crossing', 'Finishing','shooting_technique','mental'\n",
    "       'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy',\n",
    "       'LongPassing', 'BallControl',  'SprintSpeed', 'Agility',\n",
    "       'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength',\n",
    "       'LongShots', 'Aggression',  'Positioning', 'Vision',\n",
    "       'Penalties',  \n",
    "        'youth_player', 'shooting', \n",
    "       'mental', 'physique', 'Speed', 'ball_handling', 'age_based_Reactions',\n",
    "       'age_based_physique', 'age_based_shooting_technique',\n",
    "       'age_based_Stamina', 'age_based_Positioning', 'age_based_Vision',\n",
    "       'age_based_Finishing', 'age_based_BallControl']\n",
    "\n",
    "PLAYER_ATTRIBUTES = [ 'shooting', 'ShortPassing','Dribbling','BallControl',\n",
    "                     'SprintSpeed', 'Agility','Composure', 'Reactions', \n",
    "                     'Aggression', 'Interceptions', 'Positioning', 'Vision']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def find_and_append_module_path():\n",
    "    current_dir = os.getcwd()\n",
    "    substring_to_find = 'statsfaction'\n",
    "    index = current_dir.rfind(substring_to_find)\n",
    "    \n",
    "    if index != -1:\n",
    "        # Extract the directory path up to and including the last \"mypath\" occurrence\n",
    "        new_dir = current_dir[:index + (len(substring_to_find))]\n",
    "\n",
    "        # Change the current working directory to the new directory\n",
    "        os.chdir(new_dir)\n",
    "        sys.path.append(new_dir)\n",
    "        # Verify the new current directory\n",
    "        print(\"New current directory:\", os.getcwd())\n",
    "    else:\n",
    "        print(\"No 'mypath' found in the current directory\")\n",
    "find_and_append_module_path()\n",
    "os.getcwd()\n",
    "\n",
    "from src.sport_analytics.model.prepare import add_features_raw_datadf_raw\n",
    "from src.sport_analytics.model.eval import plot_feature_importance,plot_shap_summary,plot_auc_curves\n",
    "import config as CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_prime = \"\"\"\n",
    "SELECT max(Age) as prime_age,* FROM(SELECT MAX(Overall) AS PrimeOverall,*\n",
    "  FROM fifa\n",
    "  GROUP BY ID ) \n",
    "  GROUP BY ID\n",
    "  order by PrimeOverall DESC;\n",
    "\"\"\"\n",
    "\n",
    "sql_potentials = f\"\"\"\n",
    "SELECT min(Age) as potential_age,* FROM  (SELECT *,Potential as max_potential FROM fifa WHERE Potential>={TARGET_OVERALL})\n",
    "GROUP BY ID\n",
    "order by potential DESC;\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "DATABASE_PATH = \"data/sport_analytics/database/football.db\"\n",
    "# Step 1: Establish a database connection\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "df_potentials = pd.read_sql_query(sql_potentials, conn)\n",
    "df_prime = pd.read_sql_query(sql_prime, conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df_potentials = df_potentials.set_index(['ID'])\n",
    "df_prime = df_prime.set_index(['ID'])\n",
    "\n",
    "df_raw = df_potentials.join(df_prime[[\"prime_age\",\"PrimeOverall\"]])\n",
    "df_raw = df_raw.reset_index(['ID'])\n",
    "df_raw = add_features_raw_datadf_raw(df_raw)\n",
    "\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "year_to_category = {2011: 'drop', 2012: 'train', 2013: 'train', 2014: 'train', 2015: 'train', 2016: 'train', 2017: 'train', 2018: 'train', 2019: 'test', 2020: 'test', 2021: 'test', 2022: 'test', 2023: 'valid', 2024: 'valid'}\n",
    "df['set'] = df.index.get_level_values('FIFA').values\n",
    "# Apply the mapping to the \"FIFA\" column\n",
    "df['set'] = df['set'].map(year_to_category)\n",
    "\n",
    "df_potentials = df[(df.set==\"valid\")&(df.Overall<TARGET_OVERALL)&(df.Age<26)&(df.Potential>=TARGET_OVERALL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df.prime_age>df.potential_age]\n",
    "df['target'] = df.PrimeOverall\n",
    "df = df[df.potential_age<26]\n",
    "df = df[df.offense>0.5]\n",
    "print(df.target.value_counts())\n",
    "PREDICTION_NAME = \"Offense\"\n",
    "\n",
    "df_processed = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"potential_age\",\"Age\",\"prime_age\",\"max_potential\",\"Potential\",\"Overall\",\"PrimeOverall\",\"target\",\"set\",\"best_position\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True:\n",
    "#     df = df[[any(pos in i for pos in ['CF', 'LW', 'ST', 'RW']) for i in df['Position']]]\n",
    "#     df.shape\n",
    "# else:\n",
    "#     select_position = lambda x: x in [\"ST\",\"CF\",\"LW\",\"RW\"]\n",
    "#     df[\"select\"] = df['Position'].apply(select_position)\n",
    "#     df = df[df[\"select\"]]\n",
    "#     df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.columns[df_processed.isna().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_processed.fillna(0)\n",
    "df_potentials = df_potentials.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop(\"target\", axis=1, errors='ignore')\n",
    "y = df['target']  # Use df_processed here instead of df\n",
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "if False:\n",
    "    # Step 1: Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = X_train[PLAYER_ATTRIBUTES]\n",
    "    X_test = X_test[PLAYER_ATTRIBUTES]\n",
    "\n",
    "else:\n",
    "\n",
    "    X_train = X[X.set==\"train\"][PLAYER_ATTRIBUTES]\n",
    "    y_train = y[X.set==\"train\"]\n",
    "\n",
    "    X_test = X[X.set==\"test\"][PLAYER_ATTRIBUTES]\n",
    "    y_test = y[X.set==\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load your dataset or replace df_processed and df with your data\n",
    "# df_processed = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Step 2: Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 3: Fit the scaler on the training data and transform both training \n",
    "# and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "df_potentials_scaled = scaler.transform(df_potentials[PLAYER_ATTRIBUTES].fillna(0))\n",
    "\n",
    "# Step 4: Create new DataFrames with the scaled data while preserving the index and columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=PLAYER_ATTRIBUTES)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=PLAYER_ATTRIBUTES)\n",
    "df_potentials_scaled_df = pd.DataFrame(df_potentials_scaled, index=df_potentials.index, columns=PLAYER_ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparametertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 8),\n",
    "            'min_samples_split': trial.suggest_float('min_samples_split', 0.1, 1.0),  # Adjust the range\n",
    "            'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.1, 0.5),  # Adjust the range\n",
    "            'max_features': trial.suggest_float('max_features', 0.6, 1.0),\n",
    "            'criterion': 'entropy',  # or 'entropy' depending on your problem\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Implement cross-validation\n",
    "        cv_scores = cross_val_score(RandomForestClassifier(**params), X_train_scaled_df, y_train, cv=CV, scoring=SCORING)\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAMS_RF = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAMS_RF)\n",
    "    print(f\"Best {SCORING}:\", best_auc)\n",
    "else:\n",
    "    PARAMS_RF = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 12),\n",
    "            'min_samples_split': trial.suggest_float('min_samples_split', 0.1, 1.0),  # Adjust the range\n",
    "            'min_samples_leaf': trial.suggest_float('min_samples_leaf', 0.1, 0.5),  # Adjust the range\n",
    "            'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "            'criterion': 'gini',  # or 'entropy' depending on your problem\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Create the Decision Tree classifier with the given hyperparameters\n",
    "        clf = DecisionTreeClassifier(**params)\n",
    "\n",
    "        # Implement cross-validation to calculate mean AUC\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled_df, y_train, cv=CV, scoring='recall_macro')\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAM_DT = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAM_DT)\n",
    "    print(f\"Best {SCORING}:\", best_auc)\n",
    "else:\n",
    "    PARAM_DT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if HYPERTRAINING:\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to optimize\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.3),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'min_child_weight': trial.suggest_uniform('min_child_weight', 1.0, 20.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.7, 1.0),\n",
    "            'reg_alpha': trial.suggest_uniform('reg_alpha', 0.1, 1.0),\n",
    "            'reg_lambda': trial.suggest_uniform('reg_lambda', 0.01, 0.1),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        }\n",
    "        # Implement early stopping with cross-validation\n",
    "        cv_scores = []\n",
    "\n",
    "        clf = XGBClassifier(**params, random_state=42, n_jobs=-1)\n",
    "        # Implement cross-validation to calculate mean AUC\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled_df, y_train, cv=CV, scoring=SCORING)\n",
    "        mean_auc = cv_scores.mean()\n",
    "\n",
    "        return mean_auc\n",
    "    # Create an Optuna study for maximizing AUC\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)  # You can increase n_trials for more optimization\n",
    "\n",
    "    PARAM_XGB = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print(\"Best hyperparameters:\", PARAM_XGB)\n",
    "    print(f\"Best {SCORING}:\", best_auc)\n",
    "else:\n",
    "    PARAM_XGB = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "- SHAP Values for Regression Models -> DONE\n",
    "- SCV and Logistic Regression\n",
    "- Hyperparameter Tuning\n",
    "- classification model einbauen -> DONE\n",
    "- Make Usable in Deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from src.sport_analytics.model.eval import plot_feature_importance, log_metrics_in_mlflow_regression,log_metrics_in_mlflow, log_feature_list_as_artifact, plot_shap_summary\n",
    "from src.sport_analytics.utils import *\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    # 'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost Regressor': xgb.XGBRegressor(random_state=42),\n",
    "    'LightGBM Regressor': lgb.LGBMRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "# Set the experiment name\n",
    "mlflow.set_experiment(EXPERIEMENT_NAME)\n",
    "\n",
    "# Start MLflow run with a specific run name and description\n",
    "for model_name, model in regression_models.items():\n",
    "    with mlflow.start_run(run_name=RUN_NAME):\n",
    "\n",
    "        print(model_name, \"training----->\")\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"Model_Name\", model_name)\n",
    "        mlflow.log_params(model.get_params())\n",
    "\n",
    "        model.fit(X_train_scaled_df, y_train)\n",
    "       \n",
    "        y_pred = model.predict(X_test_scaled_df)\n",
    "        \n",
    "        # Log Params\n",
    "        log_feature_list_as_artifact(PLAYER_ATTRIBUTES, filename=\"feature_list.txt\")\n",
    "\n",
    "        # Create a dictionary with parameters and their values\n",
    "        params_to_log = {\n",
    "            'HYPERTRAINING': HYPERTRAINING,\n",
    "            'CV': CV,\n",
    "            'SCORING': SCORING,\n",
    "            'features_anzahl': len(PLAYER_ATTRIBUTES),\n",
    "            # 'y_train_positives': y_train.sum(),\n",
    "            # 'y_train_negatives': (~y_train).sum(),\n",
    "            # 'y_test_positives': y_test.sum(),\n",
    "            # 'y_test_negatives': (~y_test).sum(),\n",
    "            'TARGET_OVERALL': TARGET_OVERALL\n",
    "        }\n",
    "\n",
    "        # Log parameters using log_params\n",
    "        mlflow.log_params(params_to_log)\n",
    "\n",
    "        # Log artifacts\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        log_metrics_in_mlflow_regression(y_test=y_test, y_pred=y_pred)\n",
    "        log_metrics_in_mlflow(y_test=y_test>TARGET_OVERALL,y_prob=None,y_pred=y_pred>TARGET_OVERALL)\n",
    "\n",
    "        # Evaluation Plots (Note: Regression models may not have ROC curves, so adapt this as needed)\n",
    "        plot_feature_importance(model, '', top_n=20)\n",
    "        plot_shap_summary(model=model,df=X_test_scaled_df,K = 30)\n",
    "\n",
    "        # Output for quick evaluation\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        from sklearn.metrics import  classification_report\n",
    "        report = classification_report(y_test>TARGET_OVERALL, y_pred>TARGET_OVERALL)\n",
    "        print(report)\n",
    "\n",
    "     \n",
    "        regression_results[model_name] = {\n",
    "            'Model': model,\n",
    "            'Scaler': scaler,\n",
    "            'attributes': PLAYER_ATTRIBUTES,\n",
    "            'Classification Report': report,\n",
    "            'Mean Squared Error': mse,\n",
    "            'Mean Absolute Error': mae,\n",
    "            'R2 Score': r2\n",
    "        }\n",
    "\n",
    "# Evaluate and print results for each model\n",
    "for model_name, results in regression_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Mean Squared Error: {results['Mean Squared Error']:.2f}\")\n",
    "    print(f\"Mean Absolute Error: {results['Mean Absolute Error']:.2f}\")\n",
    "    print(f\"R2 Score: {results['R2 Score']:.2f}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sport_analytics.model.utils import *\n",
    "if SAVE_MODEL_NAME!=\"\":\n",
    "    save_dict_as_pickle(data_dict = regression_results, file_path=f\"{CONFIG.TRAINED_MODELS}/{SAVE_MODEL_NAME}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = regression_results['Linear Regression']['Model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.Series(my_model.predict(X_test_scaled_df),index=X_test_scaled_df.index)\n",
    "len(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_ = (result_df>=78)&(y_test>=78)\n",
    "print(bool_.sum())\n",
    "result_df[bool_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_ = (result_df<78)&(y_test>=78)\n",
    "print(bool_.sum())\n",
    "result_df[bool_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_ = (result_df>=78)&(y_test<=78)\n",
    "print(bool_.sum())\n",
    "result_df[bool_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_ = (result_df<=78)&(y_test>78)\n",
    "print(bool_.sum())\n",
    "result_df[bool_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_ = (result_df<78)&(y_test<78)\n",
    "print(bool_.sum())\n",
    "result_df[bool_].sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  classification_report\n",
    "print(classification_report(y_test>78, result_df>78))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
